# -*- coding: utf-8 -*-
"""baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E43oLfh-qXHuBilxz7mGdYM-gkZcFCDM
"""
# version august 1st

import configparser,os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
import keras
from keras.preprocessing import image# for RGB images
import os
from tqdm import tqdm
from numpy import ndarray as nd
from sklearn.model_selection import train_test_split
import cv2# cv2.imread() for grayscale images
import matplotlib.pyplot as plt
from mpl_toolkits import axes_grid1
from data_util.load_options import *
from data_util.data_load import *
from keras.utils import np_utils

CURRENT_PATH = os.getcwd()
user_config = configparser.RawConfigParser()
user_config.read(os.path.join(CURRENT_PATH, 'configuration.cfg'))
options = load_options(user_config)


def add_colorbar(im, aspect=20, pad_fraction=0.5, **kwargs):
    """
    Add a vertical color bar to an image plot.
    https://stackoverflow.com/questions/18195758/set-matplotlib-colorbar-size-to-match-graph
    """
    divider = axes_grid1.make_axes_locatable(im.axes)
    width = axes_grid1.axes_size.AxesY(im.axes, aspect=1./aspect)
    pad = axes_grid1.axes_size.Fraction(pad_fraction, width)
    current_ax = plt.gca()
    cax = divider.append_axes("right", size=width, pad=pad)
    plt.sca(current_ax)
    return im.axes.figure.colorbar(im, cax=cax, **kwargs)

data_load(options)

def uncertainty_calc(X,model,bz,sample_times):
    #MC Dropout implementation
    
    for batch_id in tqdm(range(X.shape[0] // bz)):
        # initialize our predictions
        Y_hat = np.zeros(shape=(sample_times,X.shape[0],target_height,target_width,1),dtype='float32')
        #print(Y_ts_hat.shape) 
        
        start = time.time() # MC dropout is starting !!
        
        for sample_id in range(sample_times):
            # predict stochastic dropout model T times
            Y_hat[sample_id] = model.predict(X, bz)
        # average over all passes
        prediction = Y_hat.mean(axis=0)       
        uncertainty_mc= -(prediction*np.log2(prediction) + (1-prediction)*np.log2(1-prediction)) ## entropy = -(p*np.log2(p) + (1-p)*np.log(1-p))
        end = time.time()        
        chkpnt=(end-start) # this is how long it lasts for all images of the test set, for MC Dropout-based Uncertainty Estimation
        
        #%% convert predicted mask to binary
        threshold=0.5
        prediction[prediction<threshold]=0
        prediction[prediction>=threshold]=1
        print("\n Time needed for MC Dropout-based Uncertainty Estimation  ",chkpnt)

    return prediction,uncertainty_mc

def dice2D(a,b):
    intersection = np.sum(a[b==1])
    dice = (2*intersection)/(np.sum(a)+np.sum(b))
    if (np.sum(a)+np.sum(b))==0: #black/empty masks
        dice=1.0
    return(dice)

# -*- coding: utf-8 -*-

#if used on a non-GUI server ######
import matplotlib
matplotlib.use('Agg')
###################################

import numpy as np
#import pandas as pd
import matplotlib.pyplot as plt
#import h5py
import sys
#from model import *
#from keras.utils.vis_utils import plot_model

from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.callbacks import CSVLogger
from keras.models import load_model

import time
import skimage.transform

def rotateT(X,angle):
	#rotate image tensor, TF order, single channel
	X_rot = np.zeros_like(X)
	#repeat for every channel
	for ch in np.arange(X.shape[-1]):
		#print('channel',ch)
		#repeat for every image
		for i in np.arange(X.shape[0]):
			#print('image',i)
			X_rot[i,:,:,ch] = skimage.transform.rotate(X[i,:,:,ch],angle=angle,resize=False,preserve_range=True,mode='edge')
	return(X_rot)

def shiftT(X,dx,dy):
	#rotate image tensor, TF order, single channel
	X_shift = np.zeros_like(X)
	#repeat for every image
	tform = skimage.transform.SimilarityTransform(translation=(dx, dy))
	for i in np.arange(X.shape[0]):
		#print('image',i)
		X_shift[i,:,:,:] = skimage.transform.warp(X[i,:,:,:],tform,mode='edge')
	return(X_shift)

#%%
def aug_generator(X_raw=None,Y_raw=None,
				  batch_size=4,
				  flip_axes=['x','y'],
				  rotation_angles=[5,15]):
				  #noise_gaussian_mean=0,
				  #noise_gaussian_var=1e-2):
				  #noise_snp_amount=0.05):
	
	batch_size=batch_size#recommended batch size    
	Ndatapoints = len(X_raw)
	#Naugmentations=4 #original + flip, rotation, noise_gaussian, noise_snp
	
	while(True):
		#print('start!')
		ix_randomized = np.random.choice(Ndatapoints,size=Ndatapoints,replace=False)
		ix_batches = np.array_split(ix_randomized,int(Ndatapoints/batch_size))
		for b in range(len(ix_batches)):
			#print('step',b,'of',len(ix_batches))
			ix_batch = ix_batches[b]
			current_batch_size=len(ix_batch)
			#print('size of current batch',current_batch_size)
			#print(ix_batch)
			X_batch = X_raw[ix_batch,:,:,:].copy()#.copy() to leave original unchanged
			Y_batch = Y_raw[ix_batch,:,:,:].copy()#.copy() to leave original unchanged
			
			#now do augmentation on images and masks
			#iterate over each image in the batch
			for img in range(current_batch_size):
				#print('current_image',img,': ',ix_batch[img])
				do_aug=np.random.choice([True, False],size=1)[0]#50-50 chance
				if do_aug == True:
					#print('flipping',img)
					flip_axis_selected = np.random.choice(flip_axes,1,replace=False)[0]
					if flip_axis_selected == 'x':
						flip_axis_selected = 1
					else: # 'y'
						flip_axis_selected = 0
					#flip an axis
					X_batch[img,:,:,:] = np.flip(X_batch[img,:,:,:],axis=flip_axis_selected)
					Y_batch[img,:,:,:] = np.flip(Y_batch[img,:,:,:],axis=flip_axis_selected)
					#print('Flip on axis',flip_axis_selected)
				
				do_aug=np.random.choice([True, False],size=1)[0]#50-50 chance
				if do_aug == True:
					#print('rotating',img)
					rotation_angle_selected = np.random.uniform(low=rotation_angles[0],high=rotation_angles[1],size=1)[0]
					#rotate the image
					X_batch[img,:,:,:] = rotateT(np.expand_dims(X_batch[img,:,:,:],axis=0),angle=rotation_angle_selected)
					Y_batch[img,:,:,:] = rotateT(np.expand_dims(Y_batch[img,:,:,:],axis=0),angle=rotation_angle_selected)
					#print('Rotate angle',rotation_angle_selected)
			yield(X_batch,Y_batch)
			#print('step end after',b,'of',len(ix_batches))
			
			
def train(mode,mc=False):
	  
	from keras import backend as K
	K.clear_session()
	model={}
	#load the data from already split files
	X_tr = np.load('./data/X_tr'+mode+'.npy')
	Y_tr = np.load('./data/Y_tr'+mode+'.npy')
	print('Training with '+str(X_tr.shape[0])+' images')
	X_val = np.load('./data/X_val.npy')
	Y_val = np.load('./data/Y_val.npy')
	print('Validating with '+str(X_val.shape[0])+' images')
	#%% train the model
	filepath = 'unet_div8_495K'

	#save the model when val_loss improves during training
	checkpoint = ModelCheckpoint('./trained_models/'+filepath+'_'+mode+'.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')
	#save training progress in a .csv
	csvlog = CSVLogger('./trained_models/'+filepath+'_'+mode+'_train_log.csv',append=True)
	#stop training if no improvement has been seen on val_loss for a while
	early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=8)
	batch_size=3
	
	#initialize the generator
	gen_train = aug_generator(X_tr,Y_tr,batch_size=batch_size,flip_axes=[1,2])
	#split the array and see how many splits there are to determine #steps
	steps_per_epoch_tr = len(np.array_split(np.zeros(len(X_tr)),int(len(X_tr)/batch_size)))
	
	## Step 1: Training UNET
	
	# Setup the model
	if (mc==False): 
		print("Training simple Unet")
		model=UNET(X_tr)
	else:
		print("Training Unet-mcdropout")
		model=UNET_mc(X_tr)
		
	#actually do the training
	model.fit_generator(gen_train,
						  steps_per_epoch=steps_per_epoch_tr,#the generator internally goes over the entire dataset in one iteration
						  validation_data=(X_val,Y_val),
						  epochs=80,
						  verbose=2,
						  initial_epoch=0,
						  callbacks=[checkpoint, csvlog, early_stopping])
	
	if mc==True:
		
		predictions,uncertainty_mc=uncertainty_calc(X_tr,model,1,sample_times=20)
		val_predictions,val_uncertainty_mc=uncertainty_calc(X_val,model,1,sample_times=20)
		## Step 2: Training DiceNet
		
		merged=np.concatenate((X_tr,predictions,uncertainty_mc),axis=-1)
		val_merged=np.concatenate((X_val,val_predictions,val_uncertainty_mc),axis=-1)

		model_dice=DiceNet(merged)
		#save the model when val_loss improves during training
		checkpoint = ModelCheckpoint('./trained_models/'+filepath+'_'+mode+'DiceNet.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')
		#save training progress in a .csv
		csvlog = CSVLogger('./trained_models/'+filepath+'_'+mode+'DiceNet_train_log.csv',append=True)
		#%% calculate dice
		dice = []
		dice_val=[]
		N=len(X_tr)
		for i in range(N):
			dice.append(dice2D(Y_tr[i,:,:,0],predictions[i,:,:,0]))
		#gen_train_diceNet = aug_generator(merged,np.median(dice),batch_size=batch_size,flip_axes=[1,2])
		d=np.array(dice)
		d = d.reshape(-1, 1)
		print(d.shape)
		N=len(X_val)
		for i in range(N):
			dice_val.append(dice2D(Y_val[i,:,:,0],val_predictions[i,:,:,0]))
		#gen_train_diceNet = aug_generator(merged,np.median(dice),batch_size=batch_size,flip_axes=[1,2])
		d_val=np.array(dice_val)
		d_val = d_val.reshape(-1, 1)
		model_dice.fit(merged,d,
						  #steps_per_epoch=steps_per_epoch_tr,#the generator internally goes over the entire dataset in one iteration
						  validation_data=(val_merged,dice_val),
						  epochs=60,
						  verbose=2,
						  initial_epoch=0,
						  callbacks=[checkpoint, csvlog, early_stopping])
		del model_dice
		#print(X_tr.shape,predictions.shape,uncertainty_mc.shape)
	del model

train('3',mc=True)

  
def eval(mode):
    X_ts = np.load('./data/X_ts.npy')
    Y_ts = np.load('./data/Y_ts.npy')#.astype('float32')#to match keras predicted mask

    Ntest=len(X_ts)

    df = pd.read_csv('./data/training_validation_test_splits.csv')
    well_ts = df[df['split']=='test']['well'].tolist()
    #Y_ts is a binary mask
    #np.unique(Y_ts)
    #array([ 0.,  1.], dtype=float32)

    #%% get predicted masks for test set
    model = load_model('./trained_models/unet_div8_495K'+'_'+mode+'.hdf5')
    print("Loading model: "+'./trained_models/unet_div8_495K'+'_'+mode+'.hdf5')
    
    start = time.time()
    Y_ts_hat = model.predict(X_ts,batch_size=1)
    #print(Y_ts_hat.shape)
    #max_softmax=softmaxEquation(p_hat)
    end = time.time()
    chkpnt=(end-start) # this is how long it lasts for all images of the test set, for Maximum Softmax Probability-based Uncertainty Estimation
    print("\n Time needed for Maximum Softmax Probability-based Uncertainty Estimation  ",chkpnt)

    #%% convert predicted mask to binary
    threshold=0.5
    Y_ts_hat[Y_ts_hat<threshold]=0
    Y_ts_hat[Y_ts_hat>=threshold]=1
     
    #%% calculate dice
    dice = []
    for i in range(Ntest):
        dice.append(dice2D(Y_ts[i,:,:,0],Y_ts_hat[i,:,:,0]))
    #dice = np.array(dice)
    return Y_ts_hat,dice

def eval_mc(mode,sample_times=20):
    X_ts = np.load('./data/X_ts.npy')
    Y_ts = np.load('./data/Y_ts.npy')#.astype('float32')#to match keras predicted mask
    print(X_ts.shape,Y_ts.shape)
    batch_size=1
    Ntest=len(X_ts)
    df = pd.read_csv('./data/training_validation_test_splits.csv')
    well_ts = df[df['split']=='test']['well'].tolist()
  
    #%% get predicted masks for test set
    model = load_model('./trained_models/unet_div8_495K'+'_'+mode+'.hdf5')
    print("Loading model: "+'./trained_models/unet_div8_495K'+'_'+mode+'.hdf5')
 
    pred,uncertainty_mc=uncertainty_calc(X_ts,model,batch_size,sample_times=20)
    return pred,uncertainty_mc

def eval_DiceNet(mode):
    X_ts = np.load('./data/X_ts.npy')
    Y_ts = np.load('./data/Y_ts.npy')#.astype('float32')#to match keras predicted mask

    Ntest=len(X_ts)

    #%% get predicted masks for test set
    model = load_model('./trained_models/unet_div8_495K'+'_'+mode+'DiceNet.hdf5')
    print("Loading model: "+'./trained_models/unet_div8_495K'+'_'+mode+'DiceNet.hdf5')
    print(X_ts.shape)
    ts_predictions,ts_uncertainty_mc=uncertainty_calc(X_ts,model,1,sample_times=20)
    ts_merged=np.concatenate((X_ts,ts_predictions,ts_uncertainty_mc),axis=-1)
    print(ts_merged.shape)
    qest = model.predict(ts_merged,batch_size=1)
    return qest

dice_hat=eval_DiceNet('5')

p,uncertainty_mc = eval_mc('5')

num = 3
for i in range(num):
    #sample = np.random.randint(0,len(X_ts[i,:,:,0]))
    #image = X_predict[sample]
    #gt    = Y_predict[sample]
    
    #gt    = np.squeeze(gt)
        
    #n = np.random.randint(0,num)
    fig, ax = plt.subplots(2,2,figsize=(12,6))
    
    
    #fig.suptitle('Dice: {:.2f}'.format(np.median(dice)), y=1.0, fontsize=14)
    
    cax0 = ax[0,0].imshow(X_ts[i,:,:,0])
    plt.colorbar(cax0, ax=ax[0,0])
    ax[0,0].set_title('weight mask 1')
    
    #cax1 = ax[0,1].imshow(segm[i,:,:,0])
    #plt.colorbar(cax1, ax=ax[0,1])
    #ax[0,1].set_title('Max Probability')
    #ax[0,1].set_ylabel('Prediction')
    
    cax2 = ax[0,1].imshow(p[i,:,:,0])
    plt.colorbar(cax2, ax=ax[0,1])
    ax[0,1].set_title('MC-Dropout')
    ax[0,1].set_ylabel('Prediction')
    
    cax3 = ax[1,0].imshow(X_ts[i,:,:,1])
    plt.colorbar(cax3, ax=ax[1,0])
    ax[1,0].set_title('weight mask 2')
    
    #cax4 = ax[1,1].imshow(max_softmax[i,:,:,0])
    #plt.colorbar(cax4, ax=ax[1,1])
    #ax[1,1].set_xlabel('Max Probability')
    #ax[1,1].set_ylabel('Uncertainty')

    cax5 = ax[1,1].imshow(uncertainty_mc[i,:,:,0])
    plt.colorbar(cax5,ax=ax[1,1])
    #ax[1,1].set_title('MC-Dropout')
    ax[1,1].set_ylabel('Uncertainty')
    
    #for a in ax.flatten(): a.axis('off')
    #for a in ax.flatten(): a.xaxis.set_ticks_position('none')    
    
    fig.savefig('prediction_uncertainty_{:03d}.png'.format(i), dpi=300)
    
    plt.show()
    plt.close()
    plt.clf()

def generate_plot(X,Ndatapoints,Nmodels):
  import numpy as np
  import matplotlib.pyplot as plt
#   %matplotlib inline

  connected = True
  #connected = False

  #%%

  #Ndatapoints = 20 #number of datapoints in the test set
  #Nmodels = 5 #number of models trained

  #matrix containing the data
  #rows: test set datapoints
  #columns: models
  #each elements corresponds to the score (e.g. accuracy) of a model on a dataset
  #X = np.zeros((Ndatapoints,Nmodels),dtype='float')#initialize to zeros

  np.random.seed(1)#set random seed for repeatability

  #now generate artificial data, for the sake of plotting
  #for m in range(Ndatapoints):
  #    for d in range(Nmodels):
  #        X[m,d] = np.random.uniform(low=0,high=1)

  #%% generate a plot of boxplots whose medians are connected by a line

  figure_size = 4
  fig, ax = plt.subplots(figsize=(figure_size*Nmodels,figure_size))
  box_data = X
  md = np.median(box_data,axis=0)#median
  #ax.boxplot plots each column as a separate boxplot
  bplots = ax.boxplot(box_data)

  xticks = np.arange(Nmodels)+1

  if connected == True:
      #make the boxplots transparent
      for key in bplots.keys():
          #print(key)
          for b in bplots[key]:
              b.set_alpha(0.2)
      #add a line that connects the medians of all boxplots
      ax.plot(xticks,md,marker='o',c='black',lw=5,markersize=15,label='median')
      xlab = '%'
  else:
      xlab = 'Model '

  ax.set_ylabel('Score (test set)',{'fontsize':16})
  ax.set_xlabel('Model',{'fontsize':16})
  ax.set_title('Model Performance ',{'fontsize':16})
  ax.set_ylim(0,1)

  #generate the xtick labels
  xtick_labels = []
  for m in xticks:
      xtick_labels.append(xlab+str(m))
  ax.set_xticklabels(xtick_labels,rotation = 30, ha='center',fontsize=10)

  #save the figure to disk
  if connected == True:
      plt.savefig('model_boxplots_connected.png',dpi=200,bbox_inches='tight')
  else:
      plt.savefig('model_boxplots.png',dpi=200,bbox_inches='tight')

  #%% redo the same plot without the boxplots
  # only plot a line for the medians, along with error-bars for interquantile range

  figure_size = 4
  fig, ax = plt.subplots(figsize=(figure_size*Nmodels,figure_size))
  xticks = np.arange(Nmodels)+1
  md = np.median(box_data,axis=0)
  yerr = np.zeros((2,Nmodels))
  for m in range(Nmodels):
      #plt.errorbar needs the difference between the percentile and the median
      #lower errorbar: 25th percentile
      yerr[0,m]=md[m]-np.percentile(X[:,m],25)#lower errorbar
      #upper errorbar: 75th percentile
      yerr[1,m]=np.percentile(X[:,m],75)-md[m]

  #plot the errorbars
  ax.errorbar(xticks,md,yerr,capsize=10,fmt='none',c='black')
  #fmt='none' to only plot the errorbars

  #plot the (optional) connecting line
  if connected == True:
      ax.plot(xticks,md,marker='o',c='blue',lw=5,markersize=10,label='Random query')
      xlab = '% '
  else:
      ax.scatter(xticks,md,marker='o',c='black',s=200)
      xlab = 'Model '
  plt.axhline(y=0.92, label="Full data performance",color='r', linestyle='--')
  plt.legend()

  ax.set_ylabel('Score (test set)',{'fontsize':16})
  ax.set_xlabel('Model',{'fontsize':16})
  ax.set_title('Model Performance ',{'fontsize':16})
  ax.set_ylim(0,1)

  ax.set_xticks(xticks)
  #generate the xtick labels
  xtick_labels = []
  for m in xticks:
      xtick_labels.append(str(10*m)+xlab)
  ax.set_xticklabels(xtick_labels,rotation = 30, ha='center',fontsize=10)

  if connected == True:
      plt.savefig('model_errorbars_connected.png',dpi=200,bbox_inches='tight')
  else:
      plt.savefig('model_errorbars.png',dpi=200,bbox_inches='tight')

Nmodels = 1 #number of models trained
Ndata=2 #number of modes(percentages%)

dice={}
for mode in range(1,Ndata+1):
    print("Running mode "+str(mode))
    X_ts = np.load('./data/X_ts'+str(mode)+'.npy')
    Ndatapoints = len(X_ts) #number of datapoints in the test set e.g. 2(mode1),4(mode2).. etc.
    X = np.zeros((Ndatapoints*Nmodels,Ndata),dtype='float')#initialize to zeros
    l=[]
    for i in range (Nmodels):
        train(str(mode))
        ev,dc=eval(str(mode))
        for i in dc:
            l.append(i)
        dice[mode]=l
    for d in range(mode,len(dice)+1):
        for m in range(Ndatapoints*Nmodels):
            print(d,m)
            X[m,d] = dice[d][m]

X = np.zeros((2*Nmodels,1),dtype='float')#initialize to zeros
X[1][1]

generate_plot(X)

dic={}
for mode in range (2):
  x=[]
  for i in range (4):
    x.append(i)
    dic[mode]=x
dic[1]

#save
#np.save('dice_2.npy', dice) 
#load
#read_dictionary = np.load('dice.npy',allow_pickle=True).item()

#matrix containing the data
#rows: test set datapoints
#columns: models
#each elements corresponds to the score (e.g. accuracy) of a model on a dataset
#X = np.zeros((Ndatapoints,Nmodels),dtype='float')#initialize to zeros
#array = np.array(list(dice.items()))
arr=[]
for i in range(10):
    arr.append(dice[i]) 
#print(np.asarray(arr, dtype=np.float32).shape)
#X = np.zeros((Ndatapoints,Nmodels),dtype='float')#initialize to zeros
#print(X.shape)
for d in range(Nmodels):
  for m in range(Ndatapoints):
        X[m,d] = arr[d][m]

perc=[5,10,15,20,25,30,35,40,45,50]
# %matplotlib inline
import matplotlib.pyplot as plt
plt.axhline(y=0.92, label="Full data performance",color='r', linestyle='--')
plt.plot(perc,dice_ovr,label='Random query',marker='o')
plt.xlabel('Percentage (%) of training data used')
plt.ylabel('Dice coefficient')
plt.legend()
plt.show()



